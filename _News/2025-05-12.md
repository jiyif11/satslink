---
layout: page
title: HW万核集群CloudMatrix 384
order: 1
---


# 揭秘：HW万核集群CloudMatrix 384（缩写为CM384）

原创 compute_king 



编者按：Wi-Fi 联盟，SD协会，JEDEC，PCI-SIG，USB-IF这些组织，全都把华为会员资格给撤了。简单讲，华为无法继续使用PCIe以及后续的CXL了。干脆一不做二不休，学习NVLink好榜样，自己搞了个UB一统江湖，基于UB提供的统一点对点通信能力，DDR、CPU、NPU、NIC都能像“插在同一条总线”里，实现资源池化。UB理念虽好，但独此一家的生态堪忧。如业界主流趋势将PCIe 6.0与CXL 3.0结合，实现内存一致性并支持更复杂的异构计算架构（如CPU+GPU+XPU协同），还有多厂商支持的开放标准UALink，再如百度自家的XPU Link，事实上每个国产GPU都有一个自己的Link。

### 业界主流技术对比

| 技术                | 主导方/联盟    | 关键差异点                        | 适用场景                      |
| ------------------- | -------------- | --------------------------------- | ----------------------------- |
| **NVLink**          | 英伟达         | 封闭生态、1.8TB/s带宽、最低延迟   | 高端AI训练（如Blackwell集群） |
| **UALink**          | 多厂商联盟     | 开放标准、1024节点扩展、成本低50% | 多厂商异构加速器互联          |
| **UB-Mesh**         | 华为           | 数据局部性优化、分层拓扑          | LLM训练与推理                 |
| **Infinity Fabric** | AMD            | 内存共享、PCIe Gen7结合           | AMD GPU纵向扩展               |
| **PCIe/CXL**        | 行业标准/Intel | 广泛兼容、CXL聚焦CPU生态          | 通用计算/CPU-加速器互联       |
| **UES**             | 超以太网联盟   | 横向扩展、替代InfiniBand          | 跨数据中心通信                |
| **XPU Link**        | 百度昆仑芯     | 国产自研、全互联带宽提升8倍       | 国产AI算力集群部署            |

### **文集连载预告**

国产算力核弹1 （万核集群）介绍华为CM384，采用了48台八卡昇腾910C服务器，共计48x4x64c=12288个arm v8 核（鲲鹏920 arm CPU）。 合计48 x 8 = 384个910C NPU。标准的万核➕384张卡集群。不支持CXL内存池。公开售价：500万*48*1.2（网络设备系数1.2，网络占比20%）=3个亿RMB。

国产算力核弹2 （万核集群）预告独角兽D384，采用了48台八卡P800昆仑芯服务器，共计48x256c=12288个arm v9 （Neoverse V2）核。 合计48 x 8 = 384个P800 NPU，支持CXL内存池，400G RDMA，CPU+GPU+CXL内存池协同。预计售价：1.5亿RMB。今年9月份发售。

国产算力核弹3 （千核集群）预告独角兽D72（对标GB200 NVL72），采用了9台八卡P800昆仑芯服务器，共计9x256c=2304个arm v9 （Neoverse V2）核。 合计9 x 8 = 72个P800 NPU，支持CXL内存池，400G RDMA，CPU+GPU+CXL内存池协同。售价预计：3000万RMB。今年6月份发售。

- 

### D72预售请关注 <a href="{{ site.project.repo }}">{{ site.project.repo }}</a>


外国算力核弹4 （千核集群） 现货供应英伟达的GB200 NVL 72，36 Grace CPU : 72 Blackwell GPUs。合计2,592 Arm® Neoverse V2 cores。估算450万*9=4000万。售价预计：4000万RMB。

<a href="https://www.nvidia.com/en-us/data-center/gb200-nvl72/">https://www.nvidia.com/en-us/data-center/gb200-nvl72/</a><br>


外国算力核弹5 （千核集群） 现货供应英伟达的GB300 NVL 72，36 Grace CPU : 72 Blackwell GPUs。合计2,592 Arm® Neoverse V2 cores。售价预计：5000万RMB。


<a href="https://www.nvidia.com/en-us/data-center/gb300-nvl72/?ncid=no-ncid">https://www.nvidia.com/en-us/data-center/gb300-nvl72/?ncid=no-ncid</a><br>


------

以下为正文部分，作者compute_king

- 华为CloudMatrix 384：算力界的革命性“超级核弹”Part 1 of 4我们现在身处的这个世界，早就不是那种“中心化”的“中心管控、边缘听命”的简单结构了，而是一个多层次，多极的“去中心化”大拼图。打个比方：在经济领域，有的是发达国家，有的是正在起飞的新兴市场；科技圈里，有“卷王”和创新强国，也有正努力追赶的后来者；文化层面，有全球流行风，也有本地特色味道。国家之间呢，是你中有我、我中有你。一方面大家在比拼实力，另一方面也要互相配合：比如发达国家主导某些行业的标准制定，新兴市场是新技术和资本的“实验场”，而发展中地区则通过学习带动产业升级。所以在这个多层次，融合互动的世界格局中，不管是国家，公司还是个体，都得找准自己的位置，融入进去，同时还要有足够的灵活性去应对环境变化。当然现在某些国家要完全脱钩另起炉灶，重构供应链，短期来看成功的几率也不大。说回到AI算力这个战场 ——大家都知道，Nvidia（英伟达）目前是绝对的霸主，创造了一个市值一度高达3.6万亿美元的“AI算力帝国”，在全球的数据中心市场里处于垄断地位。但别忘了，中国也有自己的力量在悄悄发力，比如华为，海光和寒武纪。尤其是华为，尽管面对各种打压和制裁，依然杀出一条血路。它家的昇腾芯片（Ascend）系列，已经越来越能打了。现在的Ascend 910C在AI推理上已经积累了一些竞争力，而下一代的Ascend 920也已经在路上了，有望增强竞争力。2025年4月10日，华为正式拿出了CloudMatrix 384这个“狠角色”：它把384颗昇腾910C芯片组成一个超级集群（SuperNode），算力高达300 PFLOPS（BF16精度），稠密（Dense）性能甚至超越了英伟达刚刚推出不久的GB200 NVL72。这不是普通升级，而是一次有可能“改写战局”的大动作。再看看寒武纪，它靠着大客户的订单，2025年一季度AI芯片收入暴涨了4230%，还第一次实现季度盈利，市值冲上3000亿元人民币，也成为国内AI芯片圈里最值钱的一家公司之一。别忘了，中国本土的7nm制程工艺良率也在提升，加上政策扶持，生态系统逐步完善，像华为，寒武纪，海光这样的选手，正在一步步夺回国内市场份额。总之，CloudMatrix 384的登场，不只是华为放出的大招，也代表了国产算力体系正在全面进化。在全球AI竞赛里，中国选手正在争夺更多话语权。聊CloudMatrix 384之前，今天我们花一点时间来聊聊华为在网络方面的技术储备。今年3月26日，华为团队发了一篇重量级论文，名字叫《UB-Mesh：一种层次化局部化的nD全连接数据中心网络架构》，英文：UB-Mesh: a Hierarchically Localized nD-FullMesh Datacenter Network Architecture。这是华为历史上联名Fellow最多的一篇论文！这篇论文的核心问题很简单：怎么多快好省地搭出一个支持上万颗甚至十万颗AI芯片的大模型训练集群？这篇论文的地址在


<a href="https://arxiv.org/html/2503.20377v1">https://arxiv.org/html/2503.20377v1</a><br>


有兴趣的小伙伴们可以去读一读。听起来很技术，我们先从这里聊开去。我来先解释一下背景，以前建立一个万卡以上规模的AI集群，需要很多的网络布局和知识。当然目前业内有一些常用的方案，比如说NVidia基于IB/RoCE网卡构建的2~3层Clos架构。GPU与GPU之间可以走多级交换（NVLink，PCIe，IB/RoCE）进行对等的数据传输。

![sample image](1.jpg)

图1，传统的数据中心网络架构NVidia的DGX SuperPOD架构就是一个非常典型的例子，有兴趣的小伙伴可以看看：

- 

<a href="https://docs.nvidia.com/dgx-superpod/reference-architecture-scalable-infrastructure-h100/latest/dgx-superpod-architecture.html">https://docs.nvidia.com/dgx-superpod/reference-architecture-scalable-infrastructure-h100/latest/dgx-superpod-architecture.html</a><br>


LLM（大语言模型）通过增加模型参数和训练数据持续提升理解，生成和推理能力。然而，这一趋势对底层训练系统和基础设施提出了更高要求，迫使下一代AI数据中心满足以下需求：R1: 大规模。随着模型规模和训练数据量的增长，需要在合理时间内完成训练所需的NPU（神经网络处理单元）或GPU数量急剧增加。例如，LLAMA-3预训练需16K GPU耗时54天。近期行业领先公司已部署10万GPU规模的AI训练系统。可扩展的基础设施是支撑LLM技术持续演进的关键。R2: 高带宽。LLM训练系统中，AI计算节点（NPU/GPU）需超过3.2Tbps的互联带宽，约为现代数据中心CPU节点互联带宽的10倍。因此，先进AI训练系统的总带宽是当前CPU基础设施即服务（IaaS）系统的10至100倍。R3: 成本效益。构建大规模AI数据中心需数十亿美元的资本支出（CapEx）。若采用传统对称Clos网络架构（图1-(a)），为满足10至100倍的总带宽增长，互联成本将同步增加10至100倍。优化网络基础设施是提升成本效益的重要途径。此外，降低运维成本（包括能耗和维护）对整体经济性同样关键。R4: 高可用性。包含10万计算节点和约百万光模块的大规模LLM训练集群面临严峻的可用性挑战。现有统计表明，即使单链路平均无故障时间（MTBF）为5年，10万GPU集群的整体MTBF仍不足30分钟。为此，网络架构设计需在提升硬件可靠性的同时，集成容错机制以应对互联、计算资源、控制系统和存储的故障。同时满足上述目标极具挑战，需要数据中心网络架构设计的范式转变。华为团队认为，下一代AI数据中心设计应遵循以下原则：P1: 流量模式驱动的网络拓扑。传统数据中心工作负载通常产生均匀随机流量，而LLM训练流量具有确定性且强数据局部性。例如，张量并行所需的集合通信占流量超50%，且多发生于8-64个相邻NPU集群内；而数据并行产生的集合通信占比不足2%，但需长距离传输。因此，需采用层次化局部化网络架构以匹配此类流量模式。

P2: 拓扑感知的计算与通信。在层次化局部化网络中高效运行LLM训练面临另一挑战：若任务分配或网络优化不足，可能导致带宽利用率低或拥塞。为此，需将并行策略、路由、集合通信、负载均衡等与网络拓扑深度协同。P3: 自愈系统实现容错。LLM训练系统需具备自愈能力以保障鲁棒性。链路故障时，路由系统应自动切换路径；NPU故障时，应无缝启用备份NPU以维持训练连续性。基于R1-R4需求和P1-P3原则，华为团队提出创新性的UB-Mesh架构。<br>

![sample image](2.jpg)

UB-Mesh采用nD全连接拓扑：递归构建从板级1D全连接、机架内2D全连接，到跨机架3D及以上全连接的层次化网络。该架构最小化传输跳数并优化单跳距离，优先直接互联而非长距离交换，减少对交换机和光模块的依赖，从而满足R1和R3。此外，支持基于LLM训练需求的层次化带宽分配（短距高带宽、长距低带宽），满足R2和R3，并遵循P1。遵循P2，华为深入优化网络与系统机制以增强UB-Mesh架构。具体包括：

1.全路径路由（APR）：结合源路由、结构化寻址与线性表查找、无死锁流控，实现自适应路由并最大化直接链路带宽利用率。

2.拓扑感知快速故障恢复：通过直接通知机制加速链路故障恢复。为满足R4和P3，UB-Mesh采用64+1高可用设计：每机架配备额外备份NPU。系统NPU故障时，备份NPU被激活以恢复功能。路由系统亦支持快速故障恢复。华为综合考虑工程约束与权衡，完成了UB-Mesh硬件与系统栈的精细设计。具体实现UB-Mesh-Pod采用4D全连接拓扑，可扩展至8K NPU，形成支持下一代AI数据中心的高带宽域。华为已经设计了核心硬件组件包括NPU，CPU，LRS（Low-Radix-Switch），HRS（ High-Radix-Switch），网卡等，通过统一总线（UB）互联。UB技术支持灵活IO资源分配和硬件资源池化，并支持跨层优化。
<br>
![sample image](3.jpg)

请注意，与采用多样化互连技术（如PCIe，NVLINK，IB和RoCE）的传统NVidia系统不同，UB-Mesh创新性地采用统一总线（Unified-Bus，UB）技术实现所有组件的互连。这种统一架构显著提升了I/O资源分配的灵活性，其点对点通信能力能够实现高效的硬件资源池化，同时为无缝跨层优化提供了技术基础。
<br>
![sample image](4.jpg)
<br>
全面评估表明：相比传统的Clos网络，UB-Mesh将高基数交换机使用量降低98%，光模块使用量减少93%，系统级成本效益提升2.04倍。在多个大语言模型（LLM）训练任务的实验中，UB-Mesh相比昂贵的Clos网络仅产生边际性能降幅（7%以内）。这种低成本与高性能的结合不仅满足当前LLM训练需求，更通过其独特的架构优势，为有效应对未来模型规模持续扩张的挑战奠定了技术基础。好吧，明天接着聊。

------

- 华为CloudMatrix 384：算力界的革命性“超级核弹”Part 2 of 4今天继续花点时间讲细一点，明天我们再聊关于CloudMatrix 384的具体实现，原谅我继续絮絮叨叨。第一，先唠几句Unified Bus（UB）先回顾下背景：2019 年，华为突然被一纸贸易禁令“点名”，结果连带着Wi-Fi 联盟，SD协会，JEDEC，PCI-SIG，USB-IF这些组织，全都把华为会员资格给撤了。简单讲，华为无法继续使用PCIe，只能另辟蹊径，于是就有了“统一总线”Unified Bus（UB）的设想 —— 想靠它顶替以太网，PCIe 这些老协议。从这个角度看，谁都想活下去，而这种“非活着不可”的劲头，往往正是科技爆发式进步的最大催化剂。所以，各位小伙伴别以为华为搞UB就是跟风AI大集群，确实不是。其实最早听到业界有人科普UB，是在2021年的APNet 2021大会，谭焜博士（当时他是华为中软院VP，并行软件实验室主任）就专门聊过 UB。有个听众当场就直接问：“关于统一总线（UB）的底层通信协议，您是否仍使用以太网/IP，还是开发了新协议？”谭博士的回应是：“我们的确开发了新协议，目标是一条通道跑到800Gbps和以上。该协议工作在网络层，像IP网络那样，但我们自己定义了新的包头，能直接嵌入拥塞信息，多路径信息，方便做多路径路由。整体设计借鉴了部分IP网络的特性，但底层按SDN（软件定义网络）来搭，更贴合大规模数据中心的需求。”听众又追问：“这意味着新协议替代了以太网和PCIe？”谭博士确认：“对，不过统一总线（UB）仍兼容IP协议运行。”当时来看，虽然还没有落地，但华为这项工作是一项非常了不起的想法和工程技术创新。第二，昨天我分享了Meta的AI集群的实现，有几个小伙伴私信一起讨论。今天展开多聊一点。和Meta类似，之前的大规模AI集群（比如说Grok3模型使用的 “Colossus”超级计算机，总共20万张H100 GPU）主要基于NVIDIA通用的8卡服务器（“Colossus”用了Dell和SMCI的8卡GPU服务器），这些服务器采用了传统的PCIE+NVLINK+RoCE的异构互联，通过Clos拓扑来构建大规模集群。这种架构在现有硬件和协议的基础上，扩展性还不错，但也有两个明显的问题：
<br>
 问题1：成本高得吓人Clos全互联拓扑需要大量的光纤来实现长距离传输，而电缆的传输距离相对有限。为了构建大规模的集群，需要投入大量的交换机和光模块。尤其是如果采用无收敛的Clos架构来实现较高的Scale-Up域带宽，互联成本在某些极端的情况下甚至会占到整个集群成本的40%以上。
<br>
问题2：异构互联下协议转换的复杂度和开销在这种架构中，NVLink域和IB/RoCE域分别对应着Scale-Up和Scale-Out的互联方式，对GPU暴露的通信接口也不同。在两个域之间传输数据时，需要在编程上进行额外的操作，这不仅增加了复杂性，还引入了不同域之间数据转发的额外开销。这个问题在DeepSeek-V3的技术报告中也有提到。大家可以找一下DeepSeek-V3 Technical Report的第20页，We aspire to see future vendors developing hardware that offloads these communication tasks from the valuable computation unit SM, serving as a GPU co-processor or a network co-processor like NVIDIA SHARP Graham et al. (2016). Furthermore, to reduce application programming complexity, we aim for this hardware to unify the IB (scale-out) and NVLink (scale-up) networks from the perspective of the computation units. With this unified interface, computation units can easily accomplish operations such as read, write, multicast, andreduce across the entire IB-NVLink-unified domain via submitting communication requests based on simple primitives.我们希望看到未来的供应商开发硬件，从宝贵的计算单元 SM 中卸载这些通信任务，作为 GPU 协处理器或网络协处理器，如 NVIDIA SHARP Graham 等人（2016 年）。此外，为了降低应用程序编程的复杂性，我们的目标是从计算单元的角度让该硬件统一 IB（横向扩展）和 NVLink（纵向扩展）网络。借助此统一接口，计算单元可以通过提交基于简单基元的通信请求，轻松完成整个 IB-NVLink 统一域中的读取、写入、多播和减少等作。也就是说：DeepSeek期望，1，未来的硬件厂商能够开发专门的硬件，将这些通信任务从宝贵的计算单元SM中卸载出来，作为GPU协处理器或类似于NVIDIA SHARP Graham et al. (2016)的网络协处理器。2，为了降低应用程序编程的复杂性，DeepSeek希望这种硬件能够从计算单元的角度统一IB（Scale Out）和NVLink（scale-up）网络。通过这种统一的接口，计算单元可以通过提交基于简单原语的通信请求，轻松地在整个IB-NVLink统一域中完成读取、写入、多播和归约等操作。当然，第2个问题在NVIDIA的NVL72和未来的NVL288超节点架构下可以得到解决，因为它们统一使用了基于多级NVSwitch的NVLink互联，构建了一个更大的Scale-Up域。不过，这种方案的成本比Clos全互联更吓人（NVL72单个机柜约300万美金，国内含税2,500万人民币，配套的水冷基础设施对机房环境也更具挑战。好吧，各位小伙伴们看看能不能跟老黄说一声，NVidia这个方案唯一就是太贵贵贵了）。第三，咱们来聊聊基于Unified Bus（UB）的“全互联”架构。刚才说了，传统的大规模AI集群里，GPU、CPU、网卡之间的互联还是各种接口拼凑 —— 节点内部走PCIe，机内多卡GPU用NVLINK互联，机器之间再跑IB或者RoCE。就像图1（a）里演示的那样：CPU、GPU、网卡各走各的道。
<br>

![sample image](5.jpg)

<br>
图1这样做虽然是沿着各家设备、各种协议各自发展走过来的“必然产物”，但缺点也很明显：
<br>
IO资源绑死了，带宽资源没法弹性调；
<br>
PCIe的更新速度跟不上NVLINK，单向带宽只有64GB/s，把整个多机扩展（Scale-Out）带到了400G 就顶不住了，想更上去只能冲PCIe瓶颈。这也就是为啥NVIDIA要自己搞Grace CPU＋NVL72超级节点，用自研CPU突破带宽限制的原因。再者，如上一节说的，不同协议间还得不停“翻译”、转换，既折腾硬件，也给编程和驱动加麻烦。要是能把所有互联统一到一条总线上，这些痛点就都能一次拿掉。这次，华为把Unified Bus（UB）拿出来，专门针对AI集群特性做了优化，CPU<-->NPU、NPU<-->NPU全都跑同一套 UB，如图1（b），可以带来几大好处：
<br>
 IO分配更自由UB跟具体用例解耦了，你想把带宽分给哪个方向（CPU→NPU、NPU→NPU 的 X/Y/Z、α/β/γ 轴），都能预留不同数量的Link灵活调，如图2（b）所示。再也不像PCIe那样死板地绑死带宽了。<br>

![sample image](6.jpg)

图2

 硬件资源能“合并当池子用”基于UB提供的统一点对点通信能力，DDR、CPU、NPU、NIC都能像“插在同一条总线”里，实现资源池化。CPU、NPU不用再一对一、非要绑死在一起，想跑哪儿给谁用都行，资源利用率自然就上来了。<br>

 系统开销更小，软件更好写协议转换不见了，驱动、通信库、上层框架都能少写一堆兼容代码，延迟、功耗都能降，调优也简单。总之，把各种异构互联“拆开重组”到UB这一条线上，AI集群的带宽弹性、资源利用率和系统复杂度都会有明显提升。第四，UB-Mesh的硬件“砖头”和“水泥”：<br>

![sample image](7.jpg)

图3在搭建UB-Mesh集群的时候，华为提供了一整套硬件组件，就好像盖楼要用的砖块和混凝土。主要有图3所示的几种：<br>

 NPU/CPU单元这俩是最基础的“砖块”。每个算力单元上都留了充足的UB IO接口，而且这些接口本身就能做转发和交换，所以在点对点直连的时候，它们自己就能把流量给扛起来，不需要专门的交换机。<br>

 UB交换机为了满足不同规模、不同带宽需求，华为还准备了两种规格的专用交换机：1，Low-Radix-Switch，LRS，利用LRS交换可以低成本地实现图2（a）所示Z轴方向的带宽池化。2，High-Radix-Switch，HRS，实现图2（a）所示α/β/γ轴方向的扩展，适合多节点的大规模部署。简而言之，NPU/CPU＋UB IO接口当“砖块”，LRS/HRS交换机当“水泥”，它们一起就能把UB-Mesh架构“搭”起来，灵活又好扩展。第五，UB-Mesh-POD集群的实现如图4（a）：先在一个机架（Rack）里，把所有8个节点，共64个NPU拉成一个二维的全互连（2D Full-Mesh），也就是每台设备都能直接跟同机架里的其他设备连上，因为在机柜内，可以用常见的直连铜缆（DAC）互联解决。<br>

![sample image](8.jpg)<br>

图4接着，需要把这个机架通过LRS交换机“延伸”到另外15个机架，这16个机架也做成2D全互连，这样一来，就相当于在一个Pod里搞出了4D的全互连，想想就像把两层二维网络叠起来，形成一个立体的网格。每个Pod里头一共有1,024个NPU，POD内距离有限，可以通过AEC的有源铜缆（AEC）互联解决。而在单个Pod外，再用HRS交换机把最多8个Pod串起来，这样就能扩展出一个8,192 个NPU 的“SuperPod（超节点）”。UB-Mesh-Pod的详细结构下面给出更 "细化" 的说明，方便小伙伴们脑补这个架构长啥样。<br>

机架内部：板卡分布如图4（b），每个机架里有： 8块NPU板卡，4 块CPU板卡，加上1块备用板。一块NPU板上挂8个NPU，一块 CPU板上挂2个CPU。<br>

 NPU 之间怎么连？如图4（b），这64个NPU（8 块NPU板 × 每板8个）直接用线缆拉成一个2D全互连，彼此都能直连。CPU如果想和NPU一起干活，就通过4个不同的交换平面（LRS 交换机）来“池化”互联。图里只画了8个LRS，其实真实环境里会更多，这里主要是示意。为了高可用，还有一个额外的NPU板当“后备”，它也通过LRS接到每个NPU上，构成“64+1”的冗余设计。怎么接入数据中心网络（DCN）？如图4（c），华为提供了两种方案：方案A：直接用UB把Pod接到DCN，这样前端网络和后端网络就融为一体了！方案B：如果客户已经有自己的公有网络生态，也可以在网络里插一个UB NIC，通过它把UB网络转换成以太网（Ethernet）。UB NIC支持UB-to-Ethernet的协议转换。这样无论是想纯用UB互联方案，还是想接入现有以太网，都能灵活应对。UB-Mesh硬件组件的实机照片昨天Part 1文中的图3展示了CPU/NPU/备份NPU板卡的实机照片，还包括机架间直连走线。主板背后、板卡之间几乎满布线缆，密度非常高，和NVidia的NVL72系统内走线类似。华为在UB-Mesh-Pod上交出的实机实物照片，正是其多年互联和NPU技术积累的成果展示。第六，测试结果华为的工程师私下透露，他们去年底就跑完了测试。可惜那会儿DeepSeek还没发布后 来爆红的DeepSeek V3和R1两个模型，所以论文里看不到它们的成绩。不过，像大家最熟的 LLAMA、GPT‑3、GPT‑4，以及MoE这类主流大模型都囊括在测试名单里。从华为公开的论文来看，不论训练哪种大模型，采用UB‑Mesh的nD‑FullMesh架构，平均都能跑到传统Clos拓扑95%左右的性能，也就是几乎不掉速。更亮眼的是，与基准Clos架构相比，UB-Mesh成功将系统中网络基础设施成本占比降至20%，这得益于节省了高性能交换机和远程光缆/模块的投入。据评估，相较于基准Clos架构，该系统节省了98%的高基数交换机和93%的光模块。按照论文提供的数据，与基准Clos架构相比，UB-Mesh以轻微的性能下降（幅度控制在7%以内）为代价，实现了2.04倍的成本效益提升。由于大幅减少交换机和光模块的使用量，UB-Mesh将网络可用性提高了7.2%。此外，UB-Mesh在多项大语言模型（LLM）训练任务中实现了95%+的线性扩展效率。对客户来说，UB-Mesh显然是个非常有意思、性价比极高的新选择。*********************作为从业者，看到这种基本上“Engineering Ready”的硬件和测试结果，不仅是对华为技术的肯定，也意味着新架构下的计算集群真正接近可商用阶段，着实令人振奋。铺垫了这么多，明天聊正题。同时放一个NVidia的NVL 288的架构图。<br>

![sample image](9.jpg)

---

- 华为CloudMatrix 384：算力界的革命性“超级核弹”Part 3 of 4今天开聊CloudMatrix 384超节点。“这是一个最好的时代，也是一个最坏的时代”（It was the best of times, it was the worst of times）出自狄更斯《双城记》的开篇。狄更斯用并置的修辞，把法国大革命前夕巴黎与伦敦的社会撕裂与希望并存的矛盾情景记录在了历史坐标上：同一时空里，有光明也有黑暗，有进步也有倒退，有信仰也有怀疑。这段话后来常被引用来描述人类在剧烈变革节点中“一体两面”的处境。回到今天的现实：技术壁垒，全球“丛林化”，霸凌和关税战是现在的语境。1，技术“高墙”林立<br>

半导体、AI芯片、5G基站等关键技术日益被视为国家安全资产。美国2019年以来对先进制程设备和GPU出口的多轮管制，致力于让全球供应链呈现“分区运行”。<br>

欧盟、日本、韩国也在本土补贴法案中加入“本土优先”条款，形成多重壁垒。2，全球社会的“丛林化”<br>

多边贸易规则弱化，单边/小多边协定盛行；地缘冲突与供应链安全叠加，企业被迫为同一产品准备“双循环”版本。<br>

资本、人才、数据开始沿着政治边界重新流动，“谁掌握算力谁说了算”成为硬逻辑。3，大国之间的科技战和关税战交错<br>

 2018年以来，从Trump第一任开启的关税战把关税壁垒从钢铝，农产品一路推到高科技产品。<br>

 “关税+禁令”双重夹击使得中国厂商必须自研替代，从EDA软件到高端光刻机，皆是如此。在这样的背景下，2025年4月10日，华为在安徽芜湖搞了个大动作 —— 主题叫“聚力共创，加速行业智能跃迁”的华为云生态大会2025正式开幕。会上，华为常务董事、华为云计算CEO张平安亲自上台，官宣了一个重磅的消息：华为在AI基础设施架构上又有了重大突破，正式发布了CloudMatrix 384超节点（下面简称CM384）而且已经在芜湖的数据中心大规模上线运行了。简单说，面对AI时代对海量算力的疯狂需求，华为这次是基于“一切可池化、一切皆对等、一切可组合”的新一代高速互联总线，搞出了CM384，打破了传统服务器那套一台一台拼起来的老办法，直接把资源从“单机单卡算力模式”拉升到了“系统及矩阵级模式”。也就是我不再和你拼单机单卡了，制程上受限制拼不过，但是我从系统之上的级别和你拼架构，拼配套，拼基建，拼电力。这也是中方的核心竞争力所在。这个CM384，主打就是三个关键词：高密度、高速率、高效率。华为宣称核心能力全面升级，不管是算力、互联带宽还是内存带宽，都做到了行业领先。各位小伙伴一定还记得我们前几天聊的UB-MESH？这次华为也是直接明确了整套系统基于UB统一通信协议，以及由此带来的内存池化和统一编址。当然笔者认为CM384是UB-MESH架构的一个早期实现，和其论文描述的最终形态还有一些差距，但其进展仍令人相当兴奋。<br>

![sample image](10.jpg)

图1，Credit to Huawei图1，感谢华为如果感兴趣的小伙伴，也可以去看看云生态大会2025自选的视频：<br>

- 

<a href="https://www.huaweicloud.com/about/hcpc2025/release.html">https://www.huaweicloud.com/about/hcpc2025/release.html</a><br>

<br>
具体聊聊现在已知和本人猜测的更多关于CloudMatrix 384超节点的信息，这里我也会引用包括SemiAnalysis在内的一些早期分析。第一，新出的CloudMatrix 384到底有多强？SemiAnalysis也在他们的长文中提到，华为的昇腾芯片他们已经很熟了。但现在这个时代，系统级的整体设计甚至比微架构细节还重要，华为在这一块做得很给力，不断在推高AI系统的整体性能。整体来看，虽然华为在芯片本身的工艺上比国际最好水平落后了大概两代，但在扩展设计方案这块，可以说已经领先于现在英伟达和AMD卖的市面版产品了。那么问题来了：这个新出的CM384到底有多强？答案是 —— 相当凶。CM384是用384颗昇腾910C芯片，通过一个全连接的拓扑结构串联起来的。设计思路也很直白：一颗芯片算力比不过英伟达Blackwell，那我直接拉芯片数量到5倍，用堆量的方式直接补回来！最终整套系统能提供300 PFLOPs的BF16精度算力，差不多是英伟达GB200 NVL72的1.7倍！而且在总内存容量上是对手的3.6倍，内存带宽也做到2.1倍。就AI系统整体能力来说，华为已经和中国本土生态一起，完成了对英伟达在某些场景的超越。当然啦，不是没有短板。如SemiAnalysis分析的，CM384的功耗确实很高，相当于GB200 NVL72的3.9倍，每单位算力的能效要低2.3倍，每TB/s内存带宽的能效低1.8倍，每TB HBM内存容量的能效也低了1.1倍。刚才聊过，即使有这些短处，CM384本质上也非常适合中方的强项，比如本土做光纤设备的实力很强，成本可控；而且华为在整个网络平面的故障预防，发现和服务做得又快又好；另外中方基建速度快，加上电力富余，功耗这个问题在中国本地用电成本相对便宜的大背景下，并无大碍，反而可以通过规模和电力优化慢慢摊平。当然，华为也还有通过提升芯片良率把规模继续拉大的潜力。第二，CloudMatrix 384的系统架构1，CPU和系统华为的CM384采用了昇腾（Ascend）910C NPU。910C基本上已经是国产NPU里的天花板了。虽然因为各种技术封锁，芯片性能受了些影响，但整体表现依然很能打。前几天听科大讯飞的电话会，科大讯飞的小伙伴们在会上确认，在他们的模型里，910C的性能在H100的80%左右。具体看配置，昇腾910C用的是Chiplet小芯片封装，配了128GB的HBM2e显存（8颗16GB的颗粒组合），算力能跑到750T的FP16精度。不过呢，它不支持更先进的FP8、FP4这些低精度数据格式。不过，910C要是跟英伟达的最新一代GB200/B200产品硬杠的话，单颗芯片的规格和性能还是差一截。受限于推文对照片数目的限制，我待会在回复里面把910C的照片和系统的框图放上来，910C整系统的互联能力确实惊人。2，CM384整体布局有多大阵仗？参考图1，另外，SemiAnalysis画了个示意图，如图2。可以看出来，CM384总共有16个机架（Rack）：其中12个是运算机架，每个机架上4台八卡昇腾910C服务器，整个CM384共有48台服务器，共计48 x 8 = 384个910C NPU。4 个交换机机架：华为在机架内集成了多个CloudEngine交换机，集中放在中间，专门负责NPU间的“Scale-Up”和“Scale-Out”扩展。

![sample image](11.jpg)<br>

图2，Credit to SemiAnalysis图 2，感谢 SemiAnalysis3，“Scale-Up”扩展网络怎么搭？为了把这些分散的节点连成一个超大规模，上百颗NPU互联的整体，华为不用传统铜缆，而是直接每颗NPU上挂7个400G的光收发器，靠光通信把不同机架里的NPU串起来，单颗NPU就有2.8 Tb/s的带宽 —— 和英伟达最新的NVL72直连铜缆效果差不多，但显然更贵更复杂。<br>

华为CloudEngine 16800模块化交换机：最多支持768个400Gb的光口，此交换机大约为32U的高度。<br>

单层扁平拓扑：所有NPU都直接通过光纤接入16800模块化交换机，在同一层次上互联不分多层级；<br>

光模块多到吓人：整套系统大概要超过5,000个光模块，光模块可靠性需要高效地保证，才能保证不掉链子。具体算一算：<br>

每颗NPU 7个400G光模块/收发器，384颗NPU就是2,688个；<br>

 交换机端也要镜像部署同样数量，所以Scale-Up网络一共用掉5,376个400G光模块。<br>

![sample image](12.jpg)

图3，Credit to SemiAnalysis图 3，感谢 SemiAnalysis4，“Scale-Out”扩展网络又是怎样？华为用了两层的全互联“胖树结构（Two-layer Leaf-Spine）”。SemiAnalysis在谈到交换这块的时候认为也用的是配备768个400G端口的CloudEngine 16800。这个几率不太大，因为4个交换机机架在放置“Scale-Up”扩展的4个大交换机就基本放满了，每个机柜只剩下最多14U的空间。显然是无法再放置一个32U高度的模块化交换机的。而华为自家有盒式的51.2T交换机 —— CloudEngine XH9230（支持128 × 400G，4U高度），对应：<br>

Leaf层：8台交换机；<br>

Spine层：4台交换机；光模块/收发器数量也好算：NPU侧384个（每NPU对应1个），Leaf层得留一半端口往上连Spine，所以要翻倍，最后“Scale-Out”网络用384 x 4 = 1,536个400G光模块/收发器。当然我也仔细看了SemiAnalysis关于各个组件的功耗计算，总体得出入不大。5，华为CM384对比NVidia NVL72华为CM384的BF16（半精度）算力是挺猛的 —— 整套系统能输出300 PFLOPs，差不多是英伟达GB200 NVL72的1.7倍。不过，劲头大了，吃得也多：CM384整机功耗接近NVL72的3.9倍，这得把数据中心的供电模组拉满。NVidia NVL72的强项，除了芯片制程领先三代（4nm vs. 7nm），HBM领先两代（HBM3e vs. HBM2），能效显著更高之外，还有软件生态已经为“稀疏计算”（Sparsity）开了挂 —— 一大堆稀疏优化算法、库、工具链都全套支持，跑一些剪枝、稀疏模型简直溜得飞起；CM384在这块，目前还只能“做中学”，暂时落后不少。再说内存和带宽，CM384能一口气给你3.6倍的总HBM容量、2.1倍的带宽，想一次性喂大模型、超大batch都没压力；NVL72则是单GPU的能力和内存带宽领先很多，所以一切以效率优先。成本方面，显然CM384成本更高。先别说芯片的良率有限，带来制造成本偏高，光是将近7,000个400G光模块，这TCO（购买价格和每个功耗~6.5W）能比NVL72的铜缆为主高不少。但考虑到华为自己垂直整合制造光模块，中方电费又便宜，基建又给力，这些事儿至少现在没那么伤脑筋。话说回来，八卦一下，早在2022年，NVIDIA秀出了 DGX H100 NVL256 “Ranger”平台。但最后这套方案停留在概念阶段，压根没量产 —— 主因是成本贵得离谱、功耗也大得吓人，还因为要堆超多光收发器＋双层网络架构，稳定性堪忧，一出问题就麻烦很大。当然，这也侧面说明，华为自己“手搓”的LPO（线性可插拔）光模块更靠谱：成本低，稳定性好，端网运维软件能力强，能顶住大规模部署的考验。简单总结：
<br>
华为CM384：统一系统架构领先，总算力猛，内存大，带宽宽，但“吃得多”“花得更多”，生态和稀疏优化还在追赶；
<br>
 Nvidia NVL72：总算力略逊（约180PFLOPs BF16），但单芯片和系统技术领先，能效领先几条街，稀疏生态成熟，适合追求高效，精细化部署的海外客户（毕竟国内买不了）。<br>

![sample image](13.jpg)<br>

图4，Credit to SemiAnalysis图 4，感谢 SemiAnalysis明天我们再聊会儿CM384的现在和将来。910C Diagram.910C 图。

![sample image](14.jpg)



910C 主板的Concept，前端Scale-Out的8个400G接口，后端Scale-Up更是排满了，总共8 x 7 = 56个。  Credit to SemiAnalysis

![sample image](15.jpg)

---

- 华为CloudMatrix 384：算力界的革命性“超级核弹” Part 4 of 4今天咱们继续聊聊CloudMatrix 384的“现在”和“未来”，看看这颗算力界的“超级核弹”还能爆发出什么新火花。第一、我们先聊聊：CloudMatrix和UB-Mesh，到底是什么关系？坊间传闻两者是同一套东西，也有人说完全没关系。那么，它们之间到底啥关系？先说笔者的结论 —— CloudMatrix和UB-Mesh更像是“同门师兄弟”关系，而不是完全一样。或者换句话说：CloudMatrix是UB1.0的实现，而UB-Mesh是UB2.0的实现。为什么这么说？咱们从几个细节上对比一下。1，硬件分组：8×8NPU、64+1高可用UB-Mesh论文明确提出，整个网络是以8×8全互联（Full Mesh）的NPU阵列为基本单元，还设计了64+1的冗余（高可用）结构，再加上CPU板和NPU板是彻底分离的。它那套架构图画得非常工整，我们之前也解释过。但你看CloudMatrix 384（以下简称CM384）的真机或者华为披露的资料，基本没照着这个来。CM384的板卡设计和NPU单系统高度都跟论文里秀的大不一样，分组方式也是4x8，64+1冗余设计在CM384的公开信息里也没看到。2，通信协议：UB统一通信协议尽管硬件形态有差异，华为在CM384发布会上却说得非常清楚：CM384是基于“UB统一通信协议”搭的。也就是说，不管架构差异如何，底层怎么布线，使用什么板卡，其实大家用的都是同一套“语言”来沟通 —— 这一点跟 UB-Mesh 论文里强调的“统一编址、内存池化”一致。所以可以这么理解：尽管CM384的网络架构，硬件形态不尽相同，但它实现了UB-Mesh的统一通信协议，所以才有了内存共享、编址一致的效果。3，UB-IO：底层通道才是关键还记得我们在Part 2里聊UB-Mesh硬件的时候，提到了UB-IO吗？它就是基于56G/112G SerDes的物理链路，上面跑的是UB统一通信协议，当然这协议也能兼容 TCP/IP；另外，按照UB-Mesh的论文，UB-IO应该还具备NPU间Full Mesh连接时点对点交换的能力，以支持全互联架构。笔者的个人猜想，华为已经把UB-IO做成了“芯粒” —— 简单说，就是把那套SerDes高速收发，以及交换能力，直接集成到芯粒里。这样，NPU+UB-IO芯粒的组合封装起来就可以解决非常多的问题。比如说，CM384里面的910C就是集成了UB-IO芯粒的版本。这样，CM384虽然在网络架构上和UB-Mesh论文不一致，但沿用了UB统一通信协议以及继承了其显著的优点。小结：硬件花样可以变，协议才是核心<br>

硬件形态：CM384 ≠ 论文里的UB-Mesh硬件；<br>

通信协议：CM384 = UB统一通信协议；<br>

底层通道（UB-IO）：华为很可能把它做成了标准“芯粒”，兼顾论文思路和工程落地。所以，如我们刚才所说，CloudMatrix和UB-Mesh不完全是一回事，更像是UB1.0和UB2.0。当然，虽然CM384跟论文里提出的理想蓝图还有差距，但其整体进展确实让人挺激动的。第二，CloudMatrix 384的“现在”1，小结一句话Ascend 910C就像在910B上“打包升级”，要想追上H100/H200除了堆算力，还得加上FP8和稀疏支持；<br>

 CM384超节点，运行DeepSeek R1的推理，保证单用户20TPS的条件下，单卡解码能冲到1920 tokens/s，基本跟H100持平；<br>

 MindSpore 2.6在软件层面全面拥抱DeepSeek-V3/R1 MoE架构，大幅提升预训练和推理吞吐，还集成了GRPO、DRO/PPO、vLLM等现代算法，开始形成软硬协同。；<br>

 出货方面，910C和CM384都蛮乐观，大客户测试稳步收尾，5，6月份就要大规模部署。2，Ascend 910C：不是发明新轮子，是“拼组合拳” 910C本质上就是把两颗910B拼一起，没有搞什么全新架构，属于“堆叠式升级”。<br>

想在国产芯片里撼动H100，单纯堆晶体管不够，还得靠FP8（让同样功耗下跑更多算力）＋稀疏（省掉那些打酱油的计算）两大招。所以业界都说，下一步910C需要对FP8来个原生支持，再上张量稀疏（Tensor Sparsity），让算力利用率再翻一番。3，CM384 超节点：解码、带宽都瞄准 H100<br>

 根据“硅基流动”的分享，CM384在跑DeepSeek R1推理时，在保证单用户20TPS（Tokens per Second）前提下，单卡解码能冲1920 tokens/s，和H100站在一个水平线。<br>

 更牛的是，CM384采用了超高带宽互联，内部节点之间的通信延迟和带宽都迈上新台阶。直接给国产算力卡插上“训练”翅膀，不只是“只读推理”那么简单。 换句话说，华为在硬件互联和系统架构上的改进，正一点点攻克“大规模大模型部署”的网络瓶颈。4，MindSpore 2.6：软件端的大集成 4月12日，昇思开发者大会上，MindSpore 2.6发布，直接对标DeepSeek V3/R1 MoE架构，预训练性能狂涨30%。<br>

 新增GRPO套件，能在千问、DeepSeek等模型上做GRPO训练，还能跑DRO/PPO强化学习，训推一体不用切来切去。<br>

 接入vLLM原生接口、DeepSeek V3/R1 Int8量化，添了10+推理融合大算子，系统吞吐2.8×飙升。<br>

 昇思还和北大、openEuler联手，把DeepSeek、MindSpore、openEuler、vLLM打包，搞成一套“大模型一体化部署”方案，行业用户可以拿来就能跑。<br>

 后面Mindspore 2.7将支持大EP并行能力，Day 0即可迁移多模态生成模型，配合CM384超节点，推理速度蹭蹭往上蹿。5，出货计划：铺得开，节奏稳客户验证：国内几大运营商和几个大厂都测过了，反馈都不错，5月份前基本收官。预计5月份之后大规模出货。主要客户：科大讯飞、蚂蚁金服、Sina Weibo、DeepSeek、奇瑞、中软国际、面壁智能、用友等等，名单还在往外加。<br>

 产品升级：据传910C最新流片的版本已经支持FP8，先开始配合科大讯飞、DeepSeek、华为云一起搞训练。<br>

 出货目标：锁定百万颗级别。CM384重点部署在芜湖、贵安、乌兰察布机房。<br>

良率情况：当前良率可控，目标是在 2025 年底前持续往上提。顺便多提一句，最近你要是留意行业信息，会发现除了地方政府，三大运营商也都在开国产算力大单：联通在上海周边，移动在长三角和东北，电信在西部，几乎都是几十亿起步。这些都是给谁？<br>

第三，CloudMatrix 384的“未来”<br>

1，Ascend 920：下一棒要更猛 性能飞跃＋HBM3 加持<br>

* 6nm＋HBM3：华为准备让SMIC的6nm工艺为Ascend 920生产，再塞上第四代高带宽内存 HBM3。想象一下，单卡FP16的算力900TFLOPS，带宽到4 TB/s，连跑最烧脑的AI模型都像刷朋友圈那么快。<br>

* 40%性能提升：官方给的数据是，920比上一代产品快40%以上。<br>
* 训练更高效：在原来架构上再打磨，训练效率比910C提高30%–40%。简直就像健身教练给你加了间歇训练，效果更惊人。挑战与机遇并存* 制程良率压力：SMIC的6nm仍未完全成熟，良率有点捉急。受限于没拿到ASML的EUV光刻机，不得不用老款 DUV，结果一步能搞定的事儿要拆成三步，良率和成本都要“捉急”一下。<br>
* HBM3供应扑朔：全球只有三星、海力士和美光能量产HBM3，中国几家还没大规模跑起来。华为得想方设法 —— 囤货、找“备用路线”、或者跟代工厂抢都在所难免。<br>
*  破局希望：别忘了，这也是千载难逢的机会。Nvidia的H20在中方市场被按住了，920一旦量产到位，就能顶上去，把空出来的市场空间占住。<br>

2，UB-Mesh＋超级节点：打破天花板 UB-Mesh的真正落地<br>

* “UB统一通信协议”：前面说过，CM384是UB1.0，920、未来的超节点也会沿用它 —— 想象一群人从小说同一种土话，换吃的、换衣服都能秒聊没障碍。<br>

* 超大规模NPU阵列：坊间有戏称，明年要上8,000卡“Super Node”，这规模简直能把Nvidia当打的NVL288的小打小闹甩脑后。训练大模型的时候再也不卡脖子。 <br>

  超节点：国产的长板<br>

* 网络带宽＋鲁棒性：回头看NVidia 2022年没搞定全光的“Ranger”，导致GB200用的是铜缆方案；但去年底今年初NVidia仍然“小翻车”，居然搞不定稳定量产。而华为却搞定了超复杂的光互连CM384。华为在网络互联know-how，事实上领先于NVidia。<br>

* 运营商＋地方政府扎堆入局：你要是留意招标，发现各地政府、三大运营商都在下大单 —— 上海周边、长三角、东北、西部…… 几百亿级别的单子，给超节点铺路。<br>

* 未来想象：想象一下，某天有个8,000卡级别的超节点上线，跑起大模型来，比现在的集群更省心、更高效。期待国产算力卡第一次在训练比赛场打头阵。<br>

3，最后一点彩蛋 “独立软硬件生态”：老黄最担心的就是美国管控越紧，中方越可能硬气起来，搞出自己一套AI软硬件生态，不再需要NVIDIA的芯片。比如华为，硬件有超大节点，软件有MindSpore、DeepSeek、vLLM那一套，一起打包成“拿来就跑”的大模型部署方案，科研所、云厂商、大学都能秒上手。从“跟跑”变成“领跑”：从Ascend 920、CM384，到UB-Mesh、超节点，再到MindSpore 2.7…… 中方正一步步把国产AI从“跟跑”变成“领跑”。记得之前有人说过：千万别低估中方工程团队在“1→10”阶段的爆发力 —— 给这些“卷王”一点空间，就能把“跟跑”变成“并跑”并“领跑”。这就是笔者眼中CM384的“将来”：硬件更猛、网络更宽、生态更全，国产 AI 的“核弹”威力，才刚刚开火。说真的，笔者无比期待2026年的到来，期待今年910C的放量，期待920的到来，期待UB-Mesh架构的产品彻底落地。写到这里，“这是一个最好的时代，也是一个最坏的时代”在笔者的眼里有了更多的含义， “最坏”：技术封锁、关税壁垒把全球化的阳面撕开了缝，企业要在不确定的法规、供应链和资本环境中裸奔。 “最好”：正因为被逼到墙角，本土创新反而获得巨大激励 —— 从芯片设计、系统互连到整机架构，华为用“系统级工程”替代“单芯片竞速”，在算力和成本之间找到别样平衡。狄更斯的话提醒我们：曲折并非终点，而是裂缝中透出的光。对中国的科技企业来说，壁垒既是束缚，也是倒逼自立的燃料；对全球产业而言，分化虽加剧，但多元技术路径也正在被重新发明。最好的时代与最坏的时代，其实只是同一张照片的正反面 —— 关键在于，站在画面里的我们如何选择姿势、创造光源。<br>

------

能看到这儿的都是行家，不卖关子了，国产千核，万核集群正在路上。<br>

**独角兽产品介绍：**

**1.核心指标：国产性能澎湃，单机可跑**671B满血4～16路

**2.产品介绍：**国产芯片，256核，16车道内存，DDR5，1T内存。

<a href="{{ site.project.repo }}/Products/Unicor-all-in-on-machine/#/products/">产品详细地址</a><br>


**3.硬件同行对比：**<br>

[新年加倍，珠峰起飞！中友会同仁服务器焕新升级，统统加倍](https://mp.weixin.qq.com/s?__biz=Mzk1NzEyNzk3OQ==&mid=2247483772&idx=1&sn=2ccfb5bc3ffc74811339b61c21f62800&scene=21#wechat_redirect)<br>

**4.国产千核、万核集群调测中，最新动态请进群或持续关注下面公众号👇👇**<br>

![sample image](20250512173516.png "内容图")<br>
