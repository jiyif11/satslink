---
layout: page
title: DeepSeek671B满血版一体机
order: 3
---

## 不要显卡！不要分布式！不要千万用户！AI平权，从珠峰开始。珠峰DeepSeek671B满血版一体机轻松助你成为10x专业人士！！
<br>

<span leaf="" style="visibility: visible;"><span textstyle="" style="font-weight: bold; visibility: visible;">前言：上文</span><a href="/News/2025-02-06/#/news/" textvalue="DeepSeek诱发低成本算力集群崛起｜轻松应对日均1000万用户访问DeepSeek R1 （671B模型）" data-itemshowtype="0" target="_blank" linktype="text" data-linktype="2" style="visibility: visible;" hasload="1"><span textstyle="" style="font-weight: bold; visibility: visible;">DeepSeek诱发低成本算力集群崛起｜轻松应对日均1000万用户访问DeepSeek R1 （671B模型）</span></a><span textstyle="" style="font-weight: bold; visibility: visible;">，有读者来信反映，千万用户的体量太大，个体户用不着，网上那些不要钱的卡得要死，真真假假，鱼目混珠，影响心情，影响寿命，我只想要一个DeepSeek 671B满血版的本地的个人助理，本地的企业助理，怎么整？</span></span><br>

废话不多说，开始<br>

<section style="visibility: visible;"><span leaf="" style="visibility: visible;"><span textstyle="" style="color: rgb(255, 41, 65); font-weight: bold; visibility: visible;">第一道菜：真假</span><a href="/News/2025-02-06/#/news/" textvalue="DeepSeek诱发低成本算力集群崛起｜轻松应对日均1000万用户访问DeepSeek R1 （671B模型）" data-itemshowtype="0" target="_blank" linktype="text" data-linktype="2" style="visibility: visible;" hasload="1"><span textstyle="" style="color: rgb(255, 41, 65); font-weight: bold; visibility: visible;">DeepSeek R1 （671B模型）</span></a></span></section><br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现在网上一个比较严重的问题是以次充好和一些无良媒体骗用户本地部署模型。这个问题其实都来源于一个原因，Deepseek 在发布 R1 的时候其实还一起放出了其他模型。R1 一起发布的还有用 R1 生成的推理数据蒸馏过的 6 个开源小模型，他们的模型名字里面也包含了 R1，但是和满血R1有很大的差别。我们知道 Deepseek  R1 之所以厉害是因为进行了 RL 也就是强化学习的训练，而了类似 DeepSeek-R1-Distill-Qwen-32B 这类模型是利用 R1 的数据在原来的模型基础上（比如 Qwen-32B）进行 STF 训练出来的。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;虽然他们训练之后相较于原来的开源模型在各项能力上获得了大幅提升，但由于没有经过 RL 强化学习的训练和较小的模型尺寸原因，模型能力是远远赶不上满血的 671B R1 模型的。<br>

![sample image](640.webp "内容图")<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;满血 R1 和蒸馏 R1 的对比： 蒸馏过的 32B 模型（假货比如Deepseek -R1-360 高速专线）和满血的 671B R1（Deepseek-R1-联网满血版），我们可以用一些热门问题来测试一下帮助大家判断。 首先是一个非常吃推理能力的问题，也是小红书热门问题，八字排盘。 因为八字排盘涉及到很多计算和推理DeepSeek-R1联网满血版模型足足思考了 121 秒两分多钟，32B 的模型仅仅思考了 14 秒，思考过程中满血在计算八字部分花了很长时间推理，32B 直接笃定的给出了八字，完全没有推理过程。 然后看另一个很热门的 Deepseek 用例，就是写文章。 DeepSeek-R1联网满血版思考了 80 秒，而 32B 思考了 10 秒，结果的差距就更加明显了，32B 的结果根本就称不上文言文。 看了这些例子，其实你大概也找到了判断的方法，首先是用一些复杂问题看思考时间，然后是对比复杂问题的回答质量。 整个使用过程中DeepSeek-R1联网满血版整个过程输出非常稳定，而且速度很快。<br> 

32B 模型：<br>

![sample image](32b.webp "内容图")<br>

671B模型：<br>

![sample image](671b.webp "内容图")<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本地版的671B满血版和deepseek官方版，铺天盖地的各种云公司推出的免费671B版“服务器繁忙，请稍后再试”的体验对比非常明显。睁大眼睛，去伪存真。<br>

### 一分钱一分货，珍惜生命，远离免费，远离低价，天天好心情！！

附：专业算命师傅排盘提示词：<br>


1. 你现在是一个中国传统八字命理的专业研究人员。
2. 你熟读穷通宝鉴这一本书籍
3. 你熟读千里命稿等有关四柱八字的一系列书籍
4. 你也熟读神峰通考等四柱八字书籍
5. 你也熟读《穷通宝鉴》《滴天髓》《三命通会》《子平真诠》《千里命稿》《五行精纪》等书籍
6. 根据“排大运分阳年、阴年。阳年：甲丙戊庚壬。阴年：乙丁己辛癸，阳年男，阴年女为顺排，阴年男，阳年女为逆排，具体排法以月干支为基准，进行顺逆，小孩交大运前，以月柱干支为大运十天干：甲乙丙丁戊己庚辛壬癸，十二地支：子丑寅卯辰巳午未申酉戌亥。
7. 我出生于xxxx年农历xx月xx日xx时xx分，请你以一个专业四柱八字研究者的角色，根据以上我所提到的书籍，及相关四柱八字的书籍和经验，对我的八字进行分析，内容越全面越好。
8. 详细分析我未来十年的财运/事业运/婚运。

<span textstyle="" style="color: rgb(255, 41, 65);font-weight: bold;">第二道菜：如何成为10X工程师</span><br>

Andrew Ng老哥不用介绍了，不认识自己上网查。<br>

![sample image](an.webp "Andrew Ng")<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在许多主要涉及应用知识或处理信息的工作中，AI将带来根本性变革。在一些岗位上，我已经看到那些精通技术的人能协调使用一系列技术工具，以与众不同的方式去完成工作，虽然他们现在可能还达不到10倍的影响力，但要实现2倍的效率提升已经不是难事。我预计这种差距会继续拉大。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;“10倍工程师”并不意味着他们写代码的速度比其他人快10倍，而是他们会在技术架构上做出更明智的决定，从而带来显著的后续影响；他们更善于发现问题、合理地确定优先级；并且与其编写1万行代码（或者标注1万个训练样本），他们也许能想到只用100行代码（或100个样本）就能完成任务的方法。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我认为“10倍市场人员”、“10倍招聘人员”、“10倍分析师”也会以类似的方式去“做不一样的事”。举例来说，也许传统的市场人员会不断重复地撰写社交媒体内容；而“10倍市场人员”可能会用AI来辅助写作，但更重要的是他们会对AI的使用进行更深入的革新。如果他们对AI的应用非常熟练——理想情况下还能自己写点代码来验证想法、自动化流程或分析数据——那么他们就能开展更多实验，更精准地把握客户需求，并生成高度个性化的内容，从而达到比传统市场人员高出数倍甚至10倍的影响力。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;同样，“10倍招聘人员”也不仅仅是用生成式AI来给候选人写邮件或总结面试内容。（在不久的将来，仅仅会使用基于提示的AI生成内容，对很多知识型岗位来说恐怕已经是“入门门槛”了。）他们很可能会整合协调一系列AI工具，高效地识别并深入研究大量候选人，从而获得远超普通招聘人员的影响力。而“10倍分析师”也绝不仅仅是让生成式AI去编辑一下他们的报告。他们可能会自己写代码，来指挥多种AI代理深入研究产品、市场和公司，进而比传统研究方式获得更有价值的结论。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2023年哈佛大学和波士顿咨询公司（BCG）的一项研究显示，如果为咨询顾问配备GPT-4，他们可以多完成12%的任务，并且完成任务的速度提高25%。这只是2023年的平均水平。随着AI技术的不断进步，如果能更高超地运用AI，所能获得的最大优势将会成倍增长。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在硅谷，我看到越来越多“AI原生”（AI-native）的团队正在重新思考流程，做出与以往截然不同的尝试。在软件工程领域，我们之所以推崇那些最优秀的工程师，是因为他们能产生极其巨大的影响力。这也激励了一代又一代的工程师不断学习和努力，因为努力能增大做出高影响力成果的概率。随着AI在更多工作岗位上带来帮助，我相信会有越来越多的人能走上类似的道路，成为“10倍专业人士”。<br>

<span textstyle="" style="color: rgb(255, 41, 65);font-weight: bold;">硬菜来了：DeepSeek AI本地推理设备搭建</span><br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们一直使用的珠峰系统获得了稳定性能表现😁 ，这套初始配置非常出色！珠峰一体机拥有者现在可以在DeepSeek R1 671B完整模型上获得10 Token/S（每秒令牌数）。完整版16K以上上下文窗口的模型体验远胜任何精简版蒸馏版，值得投入。纯CPU运行时也可同时运行视觉模型等小型模型。<br>

> 2024/02/09补充参数

空载功耗：70W（未接GPU）<br>

当前内存速度：DDR5 5200@16通道<br>

### 珠峰一体机硬件简介
 
本文没有X86，AMD，intel，Nvidia，国产显卡，外国显卡啥事，完全基于纯国产的珠峰一体机，AI平权，珠峰做起。<br>
珠峰一体机两颗128核 ARM v9 CPU，共计256个核，每个核3GHZ。这个核数傲视群雄无出其右。每个CPU有8个内存通道，两颗CPU一共16个DDR5内存通道，每个内存通道2条内存插槽，一共32个DDR5 5200的内存插槽。32个DDR5 内存插槽可轻松实现512GB-2TB内存扩容<br>
（注意：不要混用DDR4和DDR5内存！不要再提DDR4，DDR5的性能是DDR4的一倍，显卡和CPU还用着DDR4的过时了，不点名了，你们都知道我说得是谁。）<br>
<span textstyle="" style="font-size: 20px;color: rgb(255, 41, 65);font-weight: bold;">敲黑板：256个核和16个DDR5通道是AI平权的起步配置</span><br>

![sample image](cpu.webp "内容图")<br>

一体机没上图这个架势不要出来吓人了。<br>

> 珠峰一体机设备配置

* 珠峰双ARM v9 国产自主可控 CPU，5nm制程，256核*3Ghz
* 1TGB DDR5 5400 ECC内存 ，16通道插满
* 1TB NVMe 
* 1300W电源 （纯CPU推理够用，ARM V9无与伦比的高效能低功耗再接4个GPU搞训练也毫无压力）

总成本约20万+RMB ，20几万雇一个全能的AI助理，1个顶10个，不用交社保，不用交5险1金，不会抱怨，不会闹情绪，当牛马使唤，更没有劳动纠纷，还要什么自行车？<br>

> 服务器设置要点

1. 内存：16个DDR5内存通道插满，必须处于16通道满通道工作状态，充分发挥珠峰的DDR5的内存性能。
2. 主板升级：Bios升级到最新版
3. BIOS设置：SMT关闭等

## 本地AI软件部署

> Ubuntu 24基础安装

1. 通过BMC管理界面挂载Ubuntu 24.04 ISO
2. BIOS设置：UEFI/Legacy模式、性能优先配置
3. 网络配置：使用静态IP

> Ollama安装步骤


    # 安装基础工具
    sudo apt install -y htop git glances nano

    # 下载安装Ollama
    curl -L <https://ollama.com/download/ollama-linux-amd64.tgz> -o ollama.tgz
    sudo tar -C /usr -xzf ollama.tgz
    sudo useradd -r -s /bin/false -U -m -d /usr/share/ollama ollama

    # 服务配置
    sudo nano /etc/systemd/system/ollama.service
    # 添加环境变量：
    Environment=OLLAMA_NUM_PARALLEL=62
    Environment=OLLAMA_KEEP_ALIVE=3h

> OpenWEBUI部署

    # 安装Docker
    sudo apt-get install docker-ce

    # 部署Dockge管理界面
    sudo mkdir -p /opt/stacks /opt/dockge
    cd /opt/dockge
    sudo curl -O <https://raw.githubusercontent.com/louislam/dockge/master/compose.yaml>
    docker compose up -d

    # OpenWEBUI配置
    version: "3.3"
    services:
      open-webui:
        ports:
          - 7000:8080
        image: ghcr.io/open-webui/open-webui:latest

> DeepSeek R1 671B满血版性能优化建议

1. 上下文长度设置为16384（16K）
2. 线程数设为254（保留2个核心）（还用着intel/amd的X86架构，核少的可怜不要抓狂，珠峰可以满足你，赶紧换）
3. 启用mlock防止内存分页
4. 保持连接存活时间3小时

完成配置后可实现10以上 Token/S的推理速度。该方案需要专业的技术调试，珠峰DeepSeeK一体机出厂就已经调校到最佳性能，用户可获得671B完整版模型的优质体验。<br>
<span textstyle="" style="color: rgb(255, 41, 65);font-weight: bold;">我们已经推出原厂调教到最佳性能的DeepSeek671B满血版的珠峰一体机。请点击下面链接购买。</span><br>
<a href="/News/2025-02-06/#/news/" textvalue="新年加倍，珠峰起飞！中友会同仁服务器焕新升级，统统加倍" data-itemshowtype="11" target="_blank" linktype="text" data-linktype="2" hasload="1">新年加倍，珠峰起飞！中友会同仁服务器焕新升级，统统加倍</a>