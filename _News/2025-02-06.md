---
layout: page
title: DeepSeek诱发低成本算力集群崛起
order: 2
---

## DeepSeek诱发低成本算力集群崛起｜轻松应对日均1000万用户访问DeepSeek R1 （671B模型）
<br>

前言：本文基于DeepSeek R1 671B满血版讨论，7B，20B，32B，70B的蒸馏版本不在本文讨论范围以内。<br>

<span textstyle="" style="color: rgb(255, 41, 65);font-weight: bold;">第一道前菜：英伟达公司的小盒子</span><br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;英伟达公司推出的Project DIGITS搭载了英伟达GB10超级芯片，这款芯片由Blackwell GPU和Grace CPU组成，配备了128GB LPDDR5X内存和4TB NVMe SSD，能够运行超过200B大型语言模型，价格3000美金，2万多点人民币。<br>

![sample image](640.webp "内容图")<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于需要更高性能的应用场景，可以将两台Project DIGITS叠加在一起，处理多达405B的大型语言模型。<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果哪一天，这类规格的盒子在国内也上市，to B私有化部署大模型的推理成本将大大降低，也许真的触发B端的AI 大模型应用浪潮。<br>

![sample image](6.webp "内容图")<br>

这一天，也许等一年，也许等二年，但英伟达已经证明这一天必将到来！<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这个盒子确实让大家看到，也许某一天，每个打工人的桌面上都可以有一台AI超算。128G的统一内存，除了跑AI大模型，可以跑的生产力工具就多了。<br> 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;24年火过一轮的AI PC，远没这个桌面AI超算让人期待，毕竟AI PC能支撑的端侧小模型的能力太没想象空间，英伟达的这个project digits神器一出，我们需要做的只是等待，在等待的过程中为AI应用爆发做准备，成本已注定会下降到与更多应用价值匹配的程度。<br> 

### 第一次敲黑板：ARM v9的CPU+DDR5的统一内存+GPU

<span textstyle="" style="color: rgb(255, 41, 65);font-weight: bold;">第二道前菜：日均1000万用户的DeepSeek R1 （660B满血版模型）集群有多大</span><br>

深度分析下 DeepSeek R1 （671B满血版模型）用华为昇腾，推理集群大概多大？（下文都是DeePSeek671B满血版）<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;前几日，DeepSeek-R1 真的能在华为的昇腾和沐曦的AI加速卡上跑了，相关新闻自行搜索，本文以昇腾为例估算：<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;根据华为官方，Atlas（昇腾） 300I Pro 推理卡单卡拥有 140 TOPS INT8 和 70 TFLOPS FP16这个性能足够推理用，但关键的内存则使用了LPDDR4X 24 GB，总带宽204.8 GB/s。（bug来了，不支持DDR5，作为遥遥领先的爱国企业，为什么老不支持DDR5呢？如果支持DDR5就完美了，抛砖引玉，国产GPU卡还有DDR5的么？请跟帖报名。）<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这意味着运行原生的FP8版本的DeepSeek R1 （671B满血版模型）大概单卡（先不考虑装不下的问题, R1激活量是37B）是 4 token/s。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;然后还有2合一型号 Atlas 300I Duo(一块PCB焊了2个核心)，内存翻倍，LPDDR4X 96GB或48GB，总带宽408GB/s。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;那么我们计算一下如果能装下DeepSeek-R1（FP8 按照800GB计算）, 大概是 8.3 ~= 9 块卡。推理速度来到了 72 token/s 。这么看如果装得下速度其实够用（先不考虑并发）<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;然后珠峰集群单机柜是16台2U的珠峰推理机，因此单柜性能在12TB推理内存, 推理速度极限是1024 token/s，如果每个用户给60 token/s， 那么理论上单柜并发量在17左右。（单柜最大功率50.5kW）<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;那么我们来估算点更有意思的，假设有日均 1000 万用户要用DeepSeek-R1, 大概需要多少服务器?<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;假设每个用户的平均 token 输出量是 500（不考虑输入token处理），那么60 token/s 的单用户限制应该需要 8.3s才能结束生成。我们继续假设1000万用户请求平均集中在8小时内，那么每秒钟请求平均值是 10000000/(8*3600)=347.2 即每秒钟要承受348并发.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们再来计算单台服务器的QPS, 17并发/8.3s ~= 2.048.  那么最终就可以得出, 需要的服务器量为 347.2/2.048 ~=169.5 即170台服务器，约11个柜就可以！总功率来到了550kW。<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如果换更好的卡昇腾 910b 呢? 单个卡使用HBM2e 1.2TB的显存带宽, 单卡 64GB. 单机8卡, 30*64*8=15T显存，那么只需要大概30台服务器大概也就2个柜。<br>

### 第二次敲黑板，1000万用户需要30*64*8=15T显存，后面会用到！

<span textstyle="" style="color: rgb(255, 41, 65);font-weight: bold;">第三道前菜：内存显存融合，统一内存</span><br>

### 我们继续，N卡 支持内存显存统一内存，AI模型训练，推理不再会炸显存？

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;目前AI训练中，相比算力，显存大小的限制才是最头疼的地方，算力低，结果最多是慢，只要显卡够便宜，可以通过堆数量实现，但是显存不够最低线，直接就跑不了，彻底没戏。而N卡在最近的驱动中帮大家解决了CUDA内存与显存的打通。可以直接将内存作为显存，原有代码可以无缝兼容。<br>

![sample image](3.webp "内容图")<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;驱动会自动划走电脑实际内存的一半作为共享显存，比如我电脑时64G内存，显卡板载显存为4GB<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;那么对于基于CUDA的框架来说，这是一个拥有4GB高速显存+32GB慢了大约2~3倍显存的显卡。题图里可以看到，我加载了一个需要30G显存的模型，现在并不会被炸显存。而且全部是以FP32精度以CUDA运行模式载入。<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;大白话解释一下实际运行的模式，假设模型共24层，每层512MB，一共需要12GB显存。假设其他东西都不需要显存，全给模型载入用，（实际这不可能，总要留运行和其他软件的显存，而且模型具体运行也不这样，咱们只是描述一下大概的原理过程，方便理解）<br>

> 方式一

* 将前8层放入显存，占用满4GB显存
* 将其余层放入共享显存（内存）占用满8GB内存。
* 用完前8层后卸载，从内存载入9-16层到显存，占用满4GB显存。
* 用完前9-16层后卸载，从内存载入17-24层到显存，占用满4GB显存。

> 方式二

* 直接将4GB显存和8GB内存统一作为显存，但驱动会将板载的4GB显存优先作为实际显存用。类似物理内存与虚拟内存的感觉。

### 总结一下

1. 可以看到方式一中，内存to显存了1+2次，虽然内存到显存的速度还算快，不过后面这2次载入每次运算时都需要来一遍，但如果你有12GB显存，从内存to显存只需要最开始的1次。所以实际运行时，板载显存4GB+32GB共享显存，会比直接有12GB显存的显卡慢大约3倍。而方式二的速度就不好说了，我最近跑RWKV的经验来说，大概方式二比方式一在FP16精度下节约了40%用时。
2. 之前一些修改版的框架，比如torch(JTorch)也可以自动实现这个显存内存自动合用的功能，PS：其实torch应该从1.3之后的也支持，只不过需要开发者自己去实现模型分层载入显存并卸载。英伟达算是将这个东西在驱动里实现了。
3. 因为不同显卡FP32，FP16，INT8等精度下的算力差异和显存大小差别，需要测试一下是用INT8减少显存占用，减少从内存搬运到显存的用时，但会降低每层的运算速度。还是使用FP16虽然需要多从内存搬运到显存几次，但是加快每层的运算速度。到底哪一种选择会更省时间。
4. 简单的LLM类模型计算显存占用方式：FP16≈模型文件体积 X 2，in8≈模型文件体积 X 0.7。
5. 简单的真显存和内存显存融合下运算速度差距：
    1. 模型需要显存＜显卡物理显存：肯定是纯显存跑最快
    2. 显卡物理显存 X 1.5 > 模型需要显存 ＞ 显卡物理显存 X 1：大部分情况下会比纯内存+CPU要快。
    3. 显卡物理显存 X 2 > 模型需要显存＞显卡物理显存 X 1.5：不相上下，具体看你CPU和GPU运算速度的差距。
    4. 模型需要显存＞显卡物理显存 X 2：很可能用GPU还不如你用CPU+内存跑的快了。

回到20年前，intel的主板集成显卡全都具备共享显存技术，很老套很成熟。21世纪了，现在英伟达和苹果旧瓶装新酒，改个名字叫统一内存，apple的笔记本都用这个，内存和显存统一，也是DDR5。<br>

### 第三次敲黑板，内存和显存统一DDR5！

（华为鲲鹏920采用了Arm v8架构，故不支持DDR5，硬伤。如果华为是真的自研架构，真的自主可控，真的能自己维护ARM源代码，我们就期待期待华为能修改ARMV8的源代码支持DDR5吧。）<br>

<span textstyle="" style="color: rgb(255, 41, 65);font-weight: bold;">第四道前菜：RDMA AI网卡</span><br>
这道凉菜，直接上桌

![sample image](4.webp "内容图")<br>

![sample image](5.webp "内容图")<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;NX 系列标卡全面提升网络性能，实现 100Mpps 网络转发，最大 400Gbps 网络吞吐量，时延<10us；支持 IPSEC/TLS，以及 AES/SM4 算法和国密算法，大幅提升数据中心的安全性；支持高性能 RDMA，通过自研可编程拥塞控制算法平台，帮助客户根据业务类型设计和应用适合的拥塞控制算法，提升网络端到端可靠性。NX 系列标卡可提供 25G/100G/200G 多种规格，满足大中小网络规模需求。 NX 系列标卡具有良好的兼容性，支持 Linux、CGSL、欧拉、龙蜥等操作系统，兼容 X86 及 ARM CPU。NX 系列标卡为标准 PCIe 插卡，适用于通用服务器。NX 标卡可广泛用于公有云、私有云、边缘云以及智算中心的云基础设施。在通用及在智算数据中心中，NX 标卡提供高性能 RDMA 网络能力，将 deepseek AI集群的算力发挥到极致。<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;RDMA是AI集群的刚需，配的网卡不支持RDMA那就太掉价了，不提同行名字了，请自行甄别。<br>
### 第四次敲黑板，珠峰全系标配RDMA智能DPU网卡，支持deepseek的AI网络集群

<span textstyle="" style="color: rgb(255, 41, 65);font-weight: bold;">第5道前菜：AI网络</span><br>

这个也是凉菜，我的老本行了，就是把上面的AI服务器的CPU、GPU、用AI网卡连接起来，也是AI价值链里最低阶的，此菜免费赠送，收钱不好意思。<br>
上硬菜：还有比贩毒更来钱的生意么？有人说写在刑法里，错了。<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;再品尝一下第二道菜，日均1000万用户DeepSeek R1 （671B满血版模型）集群章节，前面我们提到，日均1000万用户需要17T内存，用珠峰DDR5内存+国产显卡DDR5显存，同时开启统一内存功能，珠峰DDR5内存最大支持32槽，64G*32=2T（以单条64G内存为例，还有单条128G内存的，目前太贵暂不考虑）。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;那么满足日均1000万用户访问只需要8台珠峰2U机器，半个机柜就可以搞定，8台机器，就算30万一台，投资240万你就可以摆摊开业了。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;由于统一内存技术加持，根据第一性原理，现在问题回到了这个DDR5内存到底放哪里的问题？是放在珠峰主板上，还是GPU的小身板上呢？ 毕竟珠峰插满内存那可是妥妥的2T DDR5，前面第三道菜已经说了，肯定是纯显存跑最快，可是gpu小身板上放不下那么多内存。这个悖论怎么解决？<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A要么：就学英伟达的2万块钱的小盒子统一内存，同理珠峰主板插满2T内存，同时插满八张国产DDR5卡。<br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B要么：我的很小，你要忍忍。GPU卡那个小身板，还用DDR5没有本质区别，应该用更高级更贵更快更大的内存如HBM。<br>

综上所述，日均1000用户正菜上来了（DeepSeek R1 671B满血版模型）：

* A低配：半个机柜8台珠峰，插满DDR5内存2个T，插满8张国产DDR5的GPU卡，30万一台，总价240万。
* B高配：2个机柜30台珠峰，把内存成本迁移到8张DDR5的GPU卡上。GPU读写内存少绕点路，性能会有提升，但大不了多少，如果真能大幅提高，英伟达和apple的芯片架构师也就不会搞统一内存了（我不论证了，去问硅仙人，他设计的）。30万一台，总价1000万
* C顶配：2个机柜30台珠峰，把内存成本迁移到8张HBM的GPU卡上。GPU读写内存少绕路，上N卡HBM内存或国产HBM的GPU卡，性能会提升不少，但预算要充足。

这就是没有写在刑法里的生意，开始算账。<br>
假设，日均1000万用户，一个用户一个月收一块钱，1年期。<br>

* A低配：投入250万，投入当月收回成本，月底赚4倍，到年底赚40倍，QE40。
* B中配：投入1000万，投入当月收回成本，月底赚1倍，年底赚12倍，QE12。

12个月就能赚回12倍,世上还有比贩毒更赚钱的事吗？有，这就是。能有这个QE，还需要上市去骗股民的钱么？<br>

* C高配：不讨论了，太贵没有商用的价值

<span textstyle="" style="color: rgb(255, 76, 65);font-weight: bold;">最后一次敲黑板，珠峰支持客户平滑演进，保护投资。客户根据预算和性能需求，可由低配平滑演进到高配，即使目前国产GPU卡不争气，不能老依赖Nvidia吧，胜利是属于等等党的。</span><br>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;利用全系国产自主可控芯片搭建的AI赚钱神器是不是充满想象力？ARMv9+GPU卡+DDR5+RDMA？还等什么呢？有前面美味佳肴的气氛烘托，情绪价值早已拉满，食欲大开，还不拿起鼠标订购8台珠峰AI服务器，先把场子搭起来，跑起来。<br>

- [请点击此处订购链接](https://www.runningit.cn/News/01_2025-1-26/#/news/ "中友会段总活动链接")

别问我为什么不用X86服务器，原因也在上面这个链接。本人觉得这个AI大势，已经跟老外没啥关系了。支持日均1000万用户，全系国产，逐步演进，平滑升级，还有什么好说的。<br>

![sample image](7.webp "内容图")<br>

1000万用户都支持了，如果企业自己建设私有的deepseek满血版的AI服务，支持个1000用户那更是毛毛雨，一台就够了，30万搞定。<br>
`（理论单机支持1000万/8=125万用户）`
