---
layout: page
title: KTransformers流畅运行满血DeepSeek
order: 5
---

![sample image](image.png "标题图")<br>

## 导读

在AI技术飞速发展的今天，大语言模型（LLM）的应用越来越广泛。然而，本地运行这些模型，尤其是像DeepSeek-R1这样的大模型，往往需要高性能的硬件支持，这让许多开发者和研究人员望而却步。
<br>
今天，我们为大家推荐一款由清华大学 MADSys 和 Approaching.AI 专为优化大模型本地推理体验而设计的开源框架--KTransformers。它支持在单卡24GB VRAM的GPU上运行满血版的DeepSeek-R1，性能提升高达27.79倍！
<br>
这篇文章将带您深入了解KTransformers的强大功能，以及如何轻松上手。
<br>

### KTransformers是什么？

KTransformers是一个基于Python的开源框架，专注于优化大模型的本地推理体验。它通过先进的内核优化和灵活的硬件配置策略，让开发者能够在有限的资源下实现高效的模型推理，并提供了与 Transformers 兼容的接口、符合 OpenAI 和 Ollama 标准的 RESTful API。

![sample image](image2.png "结构图")<br>

无论是单GPU、多GPU，还是CPU/GPU混合推理，KTransformers都能提供卓越的性能表现。此外，它还支持多种量化方法（如Q2K、Q3K、Q5K等），能够在不显著影响模型精度的情况下，大幅降低内存占用。<br>

### KTransformers核心功能

1. 支持DeepSeek-R1/V3本地运行<br>

KTransformers支持在单卡24GB VRAM的GPU上运行DeepSeek-R1/V3的Q4_K_M版本，性能表现如下：<br>

* Prefill Speed（tokens/s）：54.21（单节点）→ 74.362（双节点）→ 286.55（优化后）。
* Decode Speed（tokens/s）：8.73（单节点）→ 11.26（双节点）→ 13.69（优化后）。
* 相比llama.cpp，KTransformers的Prefill速度提升高达27.79倍，Decode速度提升3.03倍！

2. 支持长上下文推理<br>
KTransformers能够在单卡24GB GPU上支持128K甚至1M的长上下文推理，速度比llama.cpp快10倍以上，同时保持100%的推理精度。

3. 多GPU和异构计算支持<br>
KTransformers不仅支持多GPU并行推理，还支持CPU/GPU混合推理，充分利用硬件资源，提升推理效率。

4. 灵活的配置和优化<br>
用户可以通过简单的YAML配置文件，灵活地调整模型的优化策略，例如选择不同的量化方法或替换特定的模块。

5. 丰富的API和教程<br>
KTransformers提供了RESTful API和详细的教程文档，方便开发者快速上手。

### 如何使用KTransformers？

使用KTransformers非常简单，以下是基本步骤：
1. 安装依赖
```
    pip install ktransformers
```

2. 加载模型
```
    from transformers import AutoModelForCausalLM
    import torch

    with torch.device("meta"):
        model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)
```
3. 优化和加载模型
```
    from ktransformers import optimize_and_load_gguf

    optimize_and_load_gguf(model, optimize_rule_path, gguf_path, config)
```
4. 生成文本
```
    generated = prefill_and_generate(model, tokenizer, input_tensor.cuda(), max_new_tokens=1000)
```
> 性能对比：KTransformers vs llama.cpp

|   指标    |    llama.cpp（双节点，64核    |   KTransformers（双节点，64核）   |   提升倍数    |
|----------|----------|----------|----------|
|   Prefill Speed | 10.31 tokens/s  |  286.55 tokens/s |    27.79× |
|   Decode Speed  |    4.51 tokens/s   |  13.69 tokens/s   |   3.03×   |
{:.ui.collapsing.striped.table}

从上表可以看出，KTransformers在性能上远超llama.cpp，尤其是在Prefill阶段，速度提升了27.79倍！<br>

### KTransformers的适用场景

1. 本地开发和测试
如果您希望在本地快速开发和测试大模型，KTransformers是一个理想的选择。

2. 资源受限的环境
对于硬件资源有限的开发者，KTransformers可以通过优化和量化，让模型在有限的资源下运行得更好。

3. 高性能推理需求
如果您需要在本地实现高性能的模型推理，KTransformers的多GPU和异构计算支持能够满足您的需求。

### 如何获取KTransformers？

KTransformers的源代码和文档均是开源的，<a href="https://github.com/kvcache-ai/ktransformers" >直接访问其GitHub仓库即可</a>
